# Multi-armed bandit problem
https://en.wikipedia.org/wiki/Multi-armed_bandit

# To do:
- [x] OOP base
- [x] Epsilon-greedy
- [x] Epsilon_n-greedy
- [x] UCB1
- [x] Softmax
- [ ] Pursuit
- [ ] Reinforcement Comparison
- [ ] Thompson Sampling
- [ ] Play the winner
- [ ] One expert
- [ ] Experts
